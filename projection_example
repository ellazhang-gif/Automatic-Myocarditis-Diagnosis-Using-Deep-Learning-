from torch import nn
from torchvision.models import resnet50


class Projection(nn.Module):
  """
  Creates projection head
  Args:
    n_in (int): Number of input features
    n_hidden (int): Number of hidden features
    n_out (int): Number of output features
    use_bn (bool): Whether to use batch norm
  """
  def __init__(self, n_in: int, n_hidden: int, n_out: int,
               use_bn: bool = True):
    super().__init__()
    
    # No point in using bias if we've batch norm
    self.lin1 = nn.Linear(n_in, n_hidden, bias=not use_bn)
    self.bn = nn.BatchNorm1d(n_hidden) if use_bn else nn.Identity()
    self.relu = nn.ReLU()
    # No bias for the final linear layer
    self.lin2 = nn.Linear(n_hidden, n_out, bias=False)
  
  def forward(self, x):
    x = self.lin1(x)
    x = self.bn(x)
    x = self.relu(x)
    x = self.lin2(x)
    return x


class SimCLRModel(nn.Module):
  """
  Creates SimCLR model given encoder
  Args:
    encoder (nn.Module): Encoder
    projection_n_in (int): Number of input features of the projection head
    projection_n_hidden (int): Number of hidden features of the projection head
    projection_n_out (int): Number of output features of the projection head
    projection_use_bn (bool): Whether to use batch norm in the projection head
  """
  def __init__(self, encoder: nn.Module, projection_n_in: int = 2048,
               projection_n_hidden: int = 2048, projection_n_out: int = 2048,
               projection_use_bn: bool = True):
    super().__init__()
    
    self.encoder = encoder
    self.projection = Projection(projection_n_in, projection_n_hidden,
                                 projection_n_out, projection_use_bn)
  
  def forward(self, x):
    x = self.encoder(x)
    x = self.projection(x)
    return x
  

encoder = resnet50()
# We must remove the linear classifier after the pooling layer
encoder.fc = nn.Identity()

resnet50_simclr = SimCLRModel(encoder)

https://medium.datadriveninvestor.com/simclr-part-2-the-encoder-projection-head-and-loss-function-809a64f30d4a
